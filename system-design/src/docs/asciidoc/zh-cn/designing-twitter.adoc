[[designing-twitter]]
== Designing Twitter

设计一个类似Twitter的网络社交服务。服务中的用户可以发布推文、关注其他用户和收藏推文。难度等级：中等

[[twitter是什么]]
=== 1.Twitter是什么？

Twitter是一款在线社交网络服务，用户可以发布和阅读称之为“推文”的140字符的消息。注册的用户可以发布和阅读推文，然而未注册的用户只能阅读推文。用户通过网站界面、SMS或者手机应用程序访问Twitter。

[[需求和系统目标]]
=== 2. 需求和系统目标

我们将使用以下需求设计一个简单版本的Twitter：

功能性需求: ::

. 用户应该可以发布新的推文；
. 用户可以关注其他用户；
. 用户可以收藏推文；
. 系统可以提创建并显示用户的时间线，由用户关注的其他人的发布的推文组成，按时间顺序排列；
. 推文可以包含照片和视频。

非功能需求: ::

. 服务需要高可用；
. 系统时间线生成的可接受的延迟是200毫秒；
. 可以达到数据一致性（在牺牲可用性的条件下）；如果用户一段时间内看不到某条推文，这是可以接受的。

扩展的需求: ::

. 可以搜索推文；
. 可以在推文下回复消息；
. 热门话题 - 当前的热点话题/搜索最多的话题
. 给其他用户打标签；
. 推文通知；
. 谁关注了你？进行内容推荐？
. 精彩瞬间。

[[能力评估和约束]]
=== 3. 能力评估和约束

假设我们总共有10亿用户，日活跃用户（DAU）有2亿。也假设每天有1亿新的推文，并且平均每个用户关注200个其他用户。

一天有多少个收藏？如果，平均每个用户每天收藏5个推文，那么将会有：

[source,text]
----
200M users * 5 favorites => 1B favorites
----

我们的系统总共会生成多少推文页面？假设平均一个用户每天访问他们的时间线主页两次，并且访问其他人的主页5次。如果一个用户在每个页面上看到20个推文，那么系统每天浏览推文的数量是20B：

[source,text]
----
200M DAU * ((2 + 5) * 20 tweets) => 28B/day
----

存储评估假设每条推文有140个字符，我们需要两个字节存储一个没有被压缩的字符。假设我们需要30个字节存储每条推文的元数据（比如ID，时间戳，用户ID等）。总计需要存储量是：

[source,text]
----
100M * (280 + 30) bytes => 30GB/day
----

5的年数据需要多大的存储量呢？ 需要多少空间存储用户的数据、关注数据和收藏数据？将这些留给读者作为练习。

不是每条推文都有媒体数据，假设平均每５条推文中有一张图片，每１０条推文中有一个视频。假设平均每张图片的大小是200KB，每个视频的大小是2MB。那么每天将会有24TB的媒体数据。

[source,text]
----
(100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day
----

*带宽预估* 因为每天有24TB的数据，因此数据传输速度是290MB/秒。请记住，我们每天有280亿的推文浏览量。我们必须展示每个推文中的图片，但是假设用户平均观看到他们时间轴上的第三个视频。因此，总输出数据量是： *Bandwidth Estimates* Since total ingress is 24TB per day, this would translate into 290MB/sec.

[source,text]
----
(28B * 280 bytes) / 86400s of text => 93MB/s + (28B/5 * 200KB ) / 86400s of photos => 13GB/S + (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s
Total ~= 35GB/s
----

[[system-APIs]]
=== 4. 系统的APIs

[NOTE]
一旦我们明确了需求，那么定义系统的API接口是一个很好的主意。这应该明确说明期望系统提供的功能。

我们可以使用SOAP或者Rest API来暴露我们服务提供的功能。以下定义的是发布一个新推文接口：

[source,text]
----
tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids, maximum_results_to_return)
----

*参数：*

. api_dev_key (string): 注册账户的API开发者密钥。这样用于根据分配的配额限制用户。
. tweet_data (string): 推文的内容，通常最多为140个字符。
. tweet_location (string): 此推文所引用的可选的位置（经度，纬度）。
. user_location (string): 用户添加到推文的可选的位置（经度，纬度）。
. media_ids (number[]): 可选的推文相关的媒体数据ID的列表。（需要单独上传所有媒体照片、视频）。

*返回：* （字符串）成功地发布推文后返回访问此推文的URL地址。否则，返回一个合适的HTTP错误。

[[high-level-system-design]]
=== 5. 高级系统设计

我们需要一个可以高效地存储所有新的推文的系统，每秒存储100M/86400s => 1150个推文，并且每秒可读取28B/86400s => 325K个推文。从需求中可以看出，这将是一个读取密集型的系统。

在高层级上，我们需要多个应用服务器来处理所有的请求，并在它们前面使用负载均衡器进行流量分配。在后端，我们需要一个高效的数据库来存储所有新推文，并支持大量的读操作。我们也需要一些文件存储来保存图片和视频。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/twitter/design-twitter-high-level-system-design.png[]

尽管我们预期的写入量是每天1亿条推文，读取量是每日280亿推文。这意味着我们的系统平均每秒将接受1160条新的推文的写入和325000条读取推文的操作。该流量在一天中分布不均匀，但是在高峰期，我们应该预计每秒至少有几千条推文的写入请求和每秒大约1,000,000条读取推文的请求。在设计系统架构时，我们应该牢记这一点。

[[database-schema]]
=== 6. 数据库架构

我们需要存储的数据有用户的信息、用户的推文、用户收藏的推文和用户关注的其他用户的信息。

.Tweet
[width="25%",cols="<s,>m"]
|===
2+| Paste
|PK|TweetID: int
||Content: varchar(140)
||TweetLongitude: int
||UserLongitude: int
||CreationDate:datetime
||NumFavorites: int
|===

.User
[width="25%",cols="<s,>m"]
|===
2+| User
|PK|UserID:int
||Name:varchar(20)
||Email:varchar(32)
||DateOfBirth:datetime
||CreationDate:datetime
||LastLogin:datetime
|===

.UserFollow
[width="25%",cols="<s,>m"]
|===
2+| UserFollow
|PK|[Not supported by viewer]
|===

.Favorite
[width="25%",cols="<s,>m"]
|===
2+|Favorite
|Pk| [Not supported by viewer]
||CreationDate:datetime|
|===

对于选择使用关系型数据库还是非关系型数据库来存储数据，请参考设计Instagram中的 link:designing-instagram.adoc#数据库架构[数据库架构] 。

[[data-sharding]]
=== 7. 数据分片

由于我们每天都有大量新的推文，而且我们的读取负载也非常高，我们需要将数据分布到多台机器上，以便我们可以高效地读取/写数据。 我们有很多选择来分片我们的数据； 让我们逐个介绍：

*基于UserID的分片：* 我们可以尝试将同一用户的所有数据存储在一台服务器上。 在存储时，我们可以将 UserID 传递给哈希函数，该函数将用户映射到数据库服务器，我们将在其中存储用户的所有推文、收藏夹、关注等信息。 在查询用户的推文/关注/收藏时，我们可以询问哈希函数在哪一个数据库可以找到用户的数据，然后从那里读取。这种方法有几个问题：

. 如果用户变成热点数据怎么办？ 存储此用户数据的服务器上可能有很多查询。这种高负载会影响我们服务的性能。
. 随着时间的推移，一些用户最终可能会存储大量推文或与其他用户相比拥有大量关注。 保持不断增长的用户数据的均匀分布是相当困难的。

为了解决这些问题，我们必须重新分区或重新分配我们的数据，或者使用一致性哈希函数来解决这些问题。

*基于TweetID的分区:* 哈希函数会将TweetID映射到一个随机的服务器，我们将在其中存储该推文。我们必须查询所有服务器来搜索推文，每个服务器都会返回一组推文。中央服务器将汇总这些结果并将它们返回给用户。让我们看一下时间线生成示例； 以下是我们的系统为生成用户的时间线而必须执行的步骤数：

. 我们的应用（app）服务器会找到用户关注的所有人；
. 应用服务器会将查询发送到所有数据库服务器以查找来自这些人的推文；
. 每个数据库服务器都会找到每个用户的推文，按新近度对它们进行排序并返回排名靠前的推文；
. 应用服务器会将所有结果合并并重新排序，将排名靠前的结果返回给用户。

这种方法解决了热点用户的问题，但是，与通过 UserID 分片相比，我们必须查询所有数据库分区才能找到用户的推文，这会导致高延迟。

我们可以通过在数据库服务器前引入缓存来存储热门推文来进一步提高服务器的性能。


*基于推文创建时间分区：* 根据创建时间存储推文将使我们能够快速获取所有热门推文，并且我们只需要查询非常小的一组服务器。这里的问题是流量负载不会被分配，例如，在写入时，所有新推文都将发送到一台服务器，而其余服务器将处于空闲状态。同样，在读取时，与保存旧数据的服务器相比，保存最新数据的服务器将具有非常高的负载。

*如果我们可以组合 TweedID 和 Tweet 创建时间的分片呢？* 如果我们同时使用存储推文的创建时间和 TweetID 来反映这一点，我们可以获得这两种方法的好处。这样可以很快找到最新的推文。为此，我们必须使每个 TweetID 在系统中是唯一的，并且每个 TweetID 也应该包含一个时间戳。

我们可以使用纪元时间。假设我们的 TweetID 将有两部分：第一部分将表示纪元秒数，第二部分将是一个自动递增的序列。
因此，要创建一个新的 TweetID，我们可以采用当前纪元时间并为其附加一个自动递增的数字。
我们可以从这个 TweetID 中找出分片号并将其存储在相应的服务器。

TweetID 的大小可能是多少？
假设我们的纪元时间从今天开始，我们需要多少位来存储未来 50 年的秒数数据？

[source,text]
----
86400 sec/day * 365 (days a year) * 50 (years) => 1.6B
----

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/twitter/design-twitter-data-sharding.png[]

我们需要 31 位来存储这串数字。由于我们平均期望每秒有 1150 条新推文，我们可以分配 17 位来存储自动递增的序列；这将使 TweetID 的长度为 48 位。因此，我们每秒可以存储 (2^17 => 130K) 条新推文。可以每秒重置我们的自动递增序列。为了容错和更好的性能，我们可以有两台数据库服务器为我们生成自增键，一台生成偶数键，另一台生成奇数键。


假设当前的纪元秒数是“1483228800”，那么 TweetID 将如下所示：

[source,text]
----
1483228800 000001
1483228800 000002
1483228800 000003
1483228800 000004
…
----

如果将 TweetID 设为 64 位（8 字节）长，那么可以轻松地存储未来 100 年的推文，也可以以毫秒级为单位生成 TweetID 来存储它。

在上述方法中，我们仍然需要查询所有服务器以生成时间线数据，但我们的读取速度（和写入速度）将大大加快。

. 在创建时，由于我们没有任何二级索引，这将减少我们的写入延迟;
. 在阅读时，我们不需要过滤创建时间，因为我们的主键包含了纪元时间。

[[cache]]
=== 8. Cache

我们可以为数据库服务器引入一个缓存来缓存热门推文和用户。我们可以使用像 Memcache 这样的现成解决方案，它可以存储整个推文对象。应用程序服务器在访问数据库之前，可以快速检查缓存是否有所需的推文。
根据客户的使用模式，可以确定我们需要多少缓存服务器。


*哪种缓存替换策略最适合我们的需求？* 当缓存已满并且我们想用更新的或者更热门的推文替换旧推文时，我们将如何选择？对我们的系统来说，最近最少使用原则 (LRU) 可能是一个合理策略。根据这项策略，我们首先丢弃最近最少查看的推文。

*如何拥有一个更智能的缓存？* 如果采用 2-8 原则，即 20% 的推文产生 80% 的阅读流量，这意味着某些推文很受欢迎，以至于大多数人都会阅读它们。这表明我们可以尝试缓存每个分片 20% 的每日阅读量。

*如果我们缓存最新的数据呢？* 这种方法有益于我们的服务。假设 80% 的用户只看过去三天的推文，我们可以尝试缓存过去三天的所有推文。假设我们有专有的服务器来缓存所有用户过去三天的所有推文。如上所述，我们每天会收到1亿条推文，或者30GB新数据（不包含图片和视频）。如果想要缓存过去三天所有的推文，我们需要至少100GB大小的内存空间。这些数据可以轻松地存储到一台服务器中，但是我们应该将其复制到多台服务器中来分配所有的读取流量，以减少缓存服务器的负载。因此，无论何时生成用户的时间线，都可以查询缓存服务器是否有此用户最近的推文。如果有，那么可以从缓存服务器简单地返回所有数据。如果在缓存服务器中没有足够的推文数据，那么必须查询后端服务器以获取数据。类似地，我们可以缓存过去三天的图片和视频数据。

我们的缓存就像一个哈希表，其中“key”是“OwnerID”，“value”是一个双向链表，包含该用户在过去三天内的所有推文。由于我们想首先检索最新的数据，我们总是可以在链表的头部插入新的推文，这意味着所有较旧的推文都将在链表的尾部附近。
因此，我们可以从尾部删除推文，为新的推文腾出空间。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/twitter/design-twitter-cache.png[]

[[timeline-generation]]
=== 9. 生成时间线

有关时间线生成的详细讨论，可以参考设计Facebook的Newsfeed。

[[replication-and-fault-tolerance]]
=== 10. 备份和容错能力

由于系统的读取量大，因此可以为每个数据库分区建立多个辅助数据库服务器。辅助服务器只用于数据读取。所有的写请求将首先写入到主服务器，然后将被复制到辅助服务器。该方案为系统提供容错能力，因此每当主服务出错时，都可以将故障转移到辅助服务器。

[[负载均衡]]
=== 11. 负载均衡

可以在系统的三个地方添加负载均衡层：1）客户端和应用服务器之间；2）应用服务器和数据库备份服务器之间；3）在聚合服务器和缓存服务器之间。最初，可以采用轮询的方式；在服务器之间均等的分配传入的请求。这个 LB 实现简单，不会引入任何开销。这种方法的另一个好处是，如果服务器死了，LB 将把它从轮换中取出，并停止向它发送任何流量。Round Robin LB 的一个问题是它不会考虑服务器负载。如果服务器过载或速度慢，LB 将不会停止向该服务器发送新请求。为了解决这个问题，可以放置一个更智能的 LB 解决方案，它会定期向后端服务器查询其负载并据此调整流量。

最初，可以采用简单的轮询方式；将传入的请求平均分配给服务器。这个LB是一种简单的实现，不会引入任何额外的开销。这种方法的另一个好处是，如果一个服务器宕机了，LB 会将其踢出轮询，并停止向其发送任何请求。Round Robin LB 的一个问题是它不会考虑服务器负载。如果服务器过载或速度变慢，LB 将不会停止向该服务器发送新请求。为了解决这个问题，可以放置一个更智能的 LB 解决方案，它会定期向后端服务器查询其负载并据此调整流量。


[[监控]]
=== 12. Monitoring

拥有监控系统的能力是至关重要的。我们应该不断收集数据，以便立即了解系统是如何运行的。
我们可以收集以下指标/统计数据来了解我们服务的性能：

. 每天或每秒的新推文数，每日的峰值是多少？
. 生成时间线的统计数据，系统每天或每秒使用多少推文生成时间线？
. 用户刷新时间线的平均延迟时间。

通过监控这些统计数据，我们将了解系统是否需要更多的备份、负载均衡或者缓存。

[[扩展需求]]
=== 13. 扩展需求

*如何提供动态信息？* 从用户关注的人那里获取最新的推文，并按发布时间进行合并或排序推文。使用分页来获取或展示推文。只获取用户关注的人的最新的 N 条推文。N 取决于客户端的类型，因为在手机端展示的推文数将少于PC端。还可以缓存下一批最新的推文，以加快它们的展示速度。

或者，我们可以预先生成动态信息以提高系统效率；详情请参考 link:designing-instagram.adoc[设计Instagram]中的排名和动态消息生成。

*转发推特：* 在数据库中的每一个Tweet对象，可以存储原始Tweet的ID，而不存储这个转发推文对象的内容。

*热门话题：* 可以缓存最近N秒内频繁出现的主题标签或搜索查询，并且每隔M秒持续更新缓存的数据。可以根据推文出现或搜索查询或转发或收藏的频率对热门话题进行排序。可以将重点放在像更多人展示的主题。

*关注谁？如何进行推荐？* 此功能将提高用户的参与度。可以推荐某用户关注的人的朋友。可以高出两到三个级别，寻找名人作为推荐。可以优先推荐有更多关注者的人。

因为要随时都可以提出一些推荐信息，因此使用机器学习 (ML) 来混合数据并重新确定优先级，来推荐给用户。ML 指标可能包括最近增加关注的人，如果其他人正在关注此用户、共同的位置或兴趣等的共同关注者。

*朋友圈：* 获取过去 1 或 2 小时内不同网站的头条新闻，找出相关推文，对其进行优先排序，使用 ML（监督学习或聚类）对其进行分类（新闻、支持、经济、娱乐等）。然后我们可以将这些文章显示为朋友圈中的热门话题。

*搜索：* 使用推文的索引、排序和检索来进行搜索。类似地解决方案可以在 link:designing-twitter.adoc[设计推特搜索] 中查看.
