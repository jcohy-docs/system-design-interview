== 设计Instagram

让我们设计一个类似Instagram的图片分享服务，用户可以上传图片并分享给其他用户。类似的服务有：Flickr、Picasa。难度级别：中等。

=== 1. 什么是Instagram？

Instagram是一种社交网络服务，可让其用户上传照片和视频并分享给其他用户。Instagram用户可以选择将分享的信息设置为公开的或私密的。任何公开的共享信息对其他任意用户是可见的，而私密的共享信息只能由指定的一群人访问。Instagram还允许用户把信息分享到其他社交平台，如Facebook、Twitter、Flickr和Tumblr。

为了这个练习，我们计划设计一个更简单的版本的Instagram，用户可以分享图片，同时也可以关注其他用户。每个用户的“动态消息”将包含用户关注的所有人的最新分享的照片。

=== 2. 系统需求和目标

设计Instagram时，我们将重点关注以下需求：

==== 功能型需求

1. 用户应该可以上传、下载和浏览图片；
2. 用户可以根据照片或视频的标题进行搜索；
3. 用户可以关注其他用户；
4. 系统应该生成并显示用户的动态信息，其中包含用户关注的所有人的照片。

==== 非功能性需求

1. 我们的服务需要高可用；
2. 对于动态信息的生成，系统可接受的延迟时间是200毫秒；
3. 如果一个用户一段时间内没有看到照片，系统以执行会受到影响（为了可用性）；这事可接受的。
4. 系统应该是高可靠的，任何上传的照片或者视频都不会丢失。

不在范围内：为照片添加标签、在标签上搜索照片、对照片发表评论、将用户标记到照片、关注谁等。

=== 3. 设计注意事项

系统会有大量的读请求，因此我们将专注于构建一个可以快速检索照片的系统。

. 实际上，用户可以上传任意数量的照片。在设计此系统时，有效的存储管理应该是一个关键因素；
. 浏览照片时，应该是低延迟的；
. 数据应该百分百可靠。如果用户上传一张照片，系统将保证它永远不会丢失。

=== 4. 容量预估和约束

• 假设有5亿用户，每天有1百万活跃用户； • 每天有2百万张新照片，那么每秒有23张新照片； • 照片的平均大小 => 200KB • 一天的照片所需的存储空间为
+
[source,text]
====
 2M * 200KB => 400 GB
====

• 10年需要的总存储空间为：
+
[source,text]
====
 400GB * 365 (days a year) * 10 (years) ~= 1425TB
====

[[高级设计]]
=== 5. 高级设计

在高层次上，我们需要支持两种场景，一个是上传照片，另一个是浏览、搜索照片。我们的服务需要对象存储服务来存储照片，以及数据库服务器来存储照片相关的元数据信息。

[[数据库架构]]
=== 6. 数据库架构

[NOTE]
在面试初期阶段定义数据库表会帮助了解不同组件之间的数据流，随后将会指导数据分区。

我们需要存储用户的数据，以及他们上传的照片和他们关注的用户的信息。Photo表将会存储一张照片相关的所有数据，我们需要在 `(PhotoID, CreationDate)` 上创建索引，因为我们需要获取最近上传的照片。

.Photo
[width="25%"]
|===
2+^|Photo
>s|PK|PhotoID:int|
|userID:int|
|PhotoPath:varchar(256)|
|PhotoLatitude:int|
|PhotoLongitude:int|
|UserLatitude:int|
|UserLongitude:int|
|CreationDate:datetime|
|===

.USer
[width="25%"]
|===
2+^|User
>s|PK |UserID:int|
|Name:varchar(20)|
|Email:varchar(32)|
|DateOfBirth:datetime|
|CreationDate:datetime|
|LastLogin:datetime|
|===

.UserFollow
[width="25%"]
|===
2+^|UserFollow
>s|PK >s|UserID1:int UserID2:int
|===

存储上述模式的数据的最直接的方法是使用类似MySQL的RDBMS数据库，因为我们需要使用关联查询。但是关系型数据库也带来了各种挑战，尤其是当我们需要对数据库进行扩展时。详情请参考 SQl vs NoSQL。

可以将照片存储到类似 https://en.wikipedia.org/wiki/Apache_Hadoop[HDFS] 或 https://en.wikipedia.org/wiki/Amazon_S3[S3] 的分布式存储中

我们可以将上述模式存储在分布式键值存储中，以使用NoSQL带来的便利。与照片相关的所有元数据存储在一个表中，其中的 ’键’ 是 `PhotoID`，‘值’是包含 PhotoLocation、UserLocation、CreationTimestamp等字段的对象。

我们需要存储用户和照片之间的关系信息，以便知道照片属于谁。也要存储用户关注的其他用户的信息。对于这两个表，可以存储到类似 https://en.wikipedia.org/wiki/Apache_Cassandra[Cassandra]这样的宽列数据存储中。对于 `UserPhoto` 表，‘键’是 `UserID`，‘值’是存储在不同列的属于用户的一系列 `PhotoID`。`UserFollow` 表也是类似的存储方式。

通常，Cassandra或键值存储总是维护一定数量的副本以确保可靠性。此外，在这类数据存储中，删除操作不会立即生效，在数据永久地从系统中删除之前会保留几天（以支持数据恢复）。

=== 7. 估计数据大小

让我们预估下每张表将存储多少数据，以及存储10年的数据需要多大的存储空间。

*User*：假设每一个“int”和“dateTime”是4字节，用户表中每一行数据将会是68字节：

[source,textmate]
====
 UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes
====

如果有5亿用户，将需要32GB的存储空间。

[source,text]
====
 500 million * 68 ~= 32GB
====

*Photo*：`Photo` 表中每一行数据的大小将是284字节：

[source,text]
====
 PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes
====

如果每天上传2百万张新照片，一天将需要0.5GB大小的存储空间：

[source,text]
====
 2M * 284 bytes ~= 0.5GB per day
====

10年需要1.88TB大小的存储空间。

*UserFollow*：`UserFollow` 表中每行数据将会是8字节大小。如果我们有5亿用户，平均每个用户关注500个其他的用户。那么 `UserFollow` 表将会需要1.82TB大小的存储空间：

[source,text]
====
 500 million users * 500 followers * 8 bytes ~= 1.82TB
====

所有的表存储10年的数据需要3.7TB大小的存储空间：

[source,text]
====
 32GB + 1.88TB + 1.82TB ~= 3.7TB
====

=== 8. 组件设计

因为照片是要保存到磁盘的，因此照片上传（或写入）可能会很慢，然而照片的读取将会更快，尤其是将照片保存到缓存中时。

上传照片的用户会消费掉所有可用的链接，因为上传照片是一个很慢的过程。这意味着如果系统忙于处理所有的写请求，就无法提供“读”请求。在设计系统之前，我们应该记住Web服务器是有连接限制的。如果我们假设一个Web服务器任何时间最多可以有500个连接，那么它不能有超过500个并发上传请求或者读请求。为了解决这个瓶颈问题，我们可以把读取和写入拆分为单独的服务。我们将拥有处理读请求的专用服务器和处理写请求的不同的服务器，以确保上传照片不会占用系统的全部资源。

分离照片的读写请求也将是我们能够独立的扩展和优化这两个操作中的每一个。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/instagram/desiging-instagram-8.png[]

=== 9. 可靠性和冗余

丢失文件对我们的服务来说是不可忍受的。因此，我们将会为每个文件存储多个副本，这样如果一个存储服务器死机，可以从其他存储服务器上的文件副本中检索照片。

相同的策略也适用于系统的其他组件。如果系统想要高可用，那么需要在系统中运行服务的多个副本，这样即时一些服务宕机，系统仍然可用并运行。冗余消除了系统的单点故障。

如果在任何时间都只需要运行一个服务的实例，那么可以运行该服务的一个冗余副本，该副本不提供任何服务，但是当主服务器发生故障后，它可以接管整个系统以转移故障。

在系统中创建冗余副本可以消除单点故障，并在紧机时刻为系统提供备份和备用功能。例如，如果在生成环境运行同一个服务的两个实例，并且其中一个实例出现故障或者服务降级，则系统可以进行故障转移，使用正常运行的实例。故障转移可以自动发生或者手动执行。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/instagram/desiging-instagram-9.png[]

[[数据分片]]
=== 10. 数据分片

接下来讨论元数据分片的不同方案：

a. *基于UserID的数据分片* 假设我们基于 `UserID` 进行分片，以便于可以把用户的所有照片保存在同一个分片上。 如果一个数据库分片的大小是1TB，存储3.7TB的数据需要4个分片。为了更好的性能和可扩展性，我们使用10个分片。
+
因此可以通过 `UserID % 10` 来计算出分区号，并把数据保存到此分区中。为了使系统中的任意照片都有唯一的身份标识，可以将分区号追加到PhotoID后面。
+
*如何生成PhotoID？* 每个数据库分区可以使PhotoID自动递增，因此我们可以将 `ShardID` 追加到 `PhotoID` 后，这将使其在整个系统中是唯一的。
+
*这个分区方案有其他的问题吗？*
+
. 如何处理热门用户？一些人会关注这样的热门用户，并且许多人会看到热门用户上传的照片；
. 相较于其他用户，有些用户会上传大量照片，因此会导致存储分布不均匀；
. 如果不能把一个用户的所有图片存储到同一个分区中该怎么办？如果将一个用户的照片分开存储在不同的设备中，访问时会造成高延迟问题吗？

b. *基于 `PhotoID` 的分区* 如果我们优先生成唯一的 `PhotoID`, 然后根据 `PhotoID % 10` 计算出分区号，那么将会解决以上的问题。这个方案中，不需要把 `ShardID` 追加到 `PhotoID` 上，因为 `PhotoID` 本身在整个系统中就是唯一的。
+
*如何生成 `PhotoID`？* 在这里，我们不能在每个分区上使用自动递增序列生成 `PhotoID`，因为我们需要先知道 `PhotoID` 才能找到要存储它的分区。一种解决方案是专门使用一个单独的数据库实例来生成自动递增的ID。如果 `PhotoID` 刚好容纳64bit，那么可以定义一个只包含64bit的ID字段的表。所以每当我们想要在系统中添加一张照片时，那么我们可以在这个表中插入一行，并将该ID作为新照片的 `PhotoID`。
+
*密钥生成数据库会不会发生单点故障？* 是的，那肯定会发生。一种解决方案是可以定义两个这样的数据库，一个生成偶数ID，另一个生成奇数ID。对于MySQL，下面的脚本可以定义这样的生成序列：
+
[source,shell script]
====
    KeyGeneratingServer1:
    auto-increment-increment = 2
    auto-increment-offset = 1
====
+
[source,shell script]
====
    KeyGeneratingServer2:
    auto-increment-increment = 2
    auto-increment-offset = 2
====
+
可以在这两个数据库前面放置一个负载均衡器，以轮询的方式处理请求并处理停机时间。这两个服务器可能不同比，一个生成的密钥多于另一个，但是这对我们的系统不会造成任何问题。可以通过为系统中的 `Users`、 `Photo-Comments`或其他对象定义单独的ID表来扩展这种设计。
+
或者，我们可以实现在设计类似TinyURL的URL压缩服务中讨论的 `Key` 生成方案。
+
*如何规划系统未来的发展？* 可以使用大量的逻辑分区来适应未来数据的增长，这样一开始，在单个物理数据库服务器上保留多个逻辑分区。因为每个数据库服务器拥有多个数据库实例，因此任何服务器上的每个逻辑分区都有独立的数据库。所以每当某个数据库服务器拥有大量数据时，可以迁移一些它的逻辑分区到另一台数据库服务器。可以维护一个配置文件（或者单独的数据库）来映射逻辑分区和数据库服务器的关系；这样可以使我们轻松地移动分区。每当我们移动一个分区，只需且必须更新配置文件来通知分区的更改。

[[排名和动态消息生成]]
=== 11. 排名和动态消息生成

要给任何用户创建动态消息，需要获取用户关注的人的最新的、最受欢迎和其相关的照片。

为了简单起见，假设我们需要为用户的动态消息获取前100张照片。应用程序服务器将首先获取用户关注的人员列表，然后获取每个用户的前100张照片的元数据信息。最后，服务器将使用排序算法（基于新旧度、相似度等等）选出前100张照片，并将它们返回给用户。这种方法的一个可能的问题是高延迟，因为我们必须查询多个表并对结果进行排序、合并以及再排序。为了提升效率，我们可以预先生成动态消息，并将其存储在单独的表中。

*预先生成动态消息：* 使用专门的服务器持续不断地生成用户的动态消息，并将其存储在 `UserNewsFeed` 表中。所以每当任何用户需要为其动态消息获取最新的照片时，我们都会简单地查询这张表，然后返回查询结果给用户。

每当这些服务器需要生成用户的动态消息时，将首选查询 `UserNewsFeed` 表以查找上次为该用户生成的动态消息的时间。然后，从那个时间起生成新的动态消息（根据以上提及的步骤）。

*向用户发送动态消息的方法有哪些？*

. *拉取:* 客户端可以定期或在需要时手动从服务器拉取新闻提要内容。这个方法可能存在的问题时：
+
.. 在客户端发起请求之前，可能不会向用户显示新数据；
+
.. 如果没有新的数据，大多数拉取请求将会得到一个空响应结果。

. *推送：* 当新数据可用时，服务器主动向用户推送它。为了更有效的管理这一点，用户必须与服务器保持一个长轮询请求以接受服务器的消息推送。这种方法可能存在的问题是，用户关注许多人或者一个拥有数百万关注者的名人用户，在这种情况下，服务器必须非常频繁地向用户推送数据。

. *混合：* 可以采用一种混合的方式。拥有大量关注者的用户采用拉取的方法，对于只有几百上千关注者的用户由服务器主动推送其动态消息。另一种方法可以是服务器向更新不超过一定频率的用户主动推送数据，让有大量关注者或大批量更新的用户定期拉取数据。

对于动态消息生成的详细讨论，请参考设计Facebook的动态消息。

=== 12. 使用分片数据生成动态信息

为任何用户创建动态消息的最重要的需求之一是从用户关注的所有人那里获取最新照片。为此，我们需要一个机制来根据创建时间对照片进行排序。为了有效地做到这一点，可以将照片的创建时间作为 `PhotoID` 的一部分。因为 `PhotoID` 上有主键索引，因此可以很快地查询到最新的 `PhotoID`。

可以为此使用纪元时间。假设 `PhotoID` 有两部分。第一部分将代表纪元时间，第二部分将是一个自动递增的序列。所以为了生成一个新的的 `PhotoID`，可以获取当前的纪元时间并追加一个来自密钥生成数据库的自动递增序列。可以根据这个 `PhotoID` 找出分片号（PhotoID % 10），并将照片存储在这个分片中。

*PhotoID的大小是多少？* 假设纪元时间是从今天开始，那么需要多少bit存储未来50年的秒数？

[source,text]
====
 86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds
====

需要31bit的空间来存储这个数字。因为平均而言，我们预期每秒有23张新照片，可以分配9bit存储自动递增序列。因此每秒可以存储（2^9 => 512）张新照片。可以每秒重置自动递增序列。

将会在设计Twitter中的‘数据分片’中详细讨论此技术。

=== 13. 缓存和负载均衡

我们的服务需要一个大规模的照片交付系统来服务分布全球的用户。应该使用大量分布在不同地理位置的照片缓存服务器并使用CDN（有关详细信息，请参阅缓存）将其相关的内容推送到临近的用户。

可以为元数据服务器引入缓存技术来存储热点数据。可以使用Memcache缓存数据，应用服务器在查询数据库之前先快速检查缓存中是否有所需的数据。最近最少使用（LRU）对我们的系统来说是一个合适的缓存驱逐策略。在此策略下，将首先删除缓存中最近最少查询的数据。

*如何构建更只能的缓存？* 如果我们采用80-20规则，即每天20%的照片查看量将会产生80%的流量，这意味着某些照片非常受欢迎。以至于大多数人会查阅它们。这表明我们可以缓存20%每日照片查询量和元数据。